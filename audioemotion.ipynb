{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#import libraries\nimport librosa\nimport librosa.display\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom matplotlib.pyplot import specgram\nimport glob\nfrom sklearn.metrics import confusion_matrix\nimport os\nimport sys\nimport warnings\nimport IPython.display as ipd\nimport seaborn as sns\nimport glob\nimport pickle\nimport json\n\nif not sys.warnoptions :\n    warnings.simplefilter('ignore')\nwarnings.filterwarnings(\"ignore\", category  = DeprecationWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are taking 4 datasets, which are SAVEE dataset, RAVDESS dataset, TESS dataset and CREMA - D dataset, these all are in kaggle. "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"TESS = \"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\nRAV = \"../input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\nSAVEE = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/\"\nCREMA = \"../input/cremad/AudioWAV/\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SAVEE DATASET"},{"metadata":{"trusted":true},"cell_type":"code","source":"dir_list = os.listdir(SAVEE)\ndir_list[0:5]\n\n#for i in dir_list:\n#    print(i[-8 : -6])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SAVEE datasets\nemotions are as follows\n* a : anger\n* d : disgust\n* f : fear\n* h : happiness\n* sa: sadness\n* su: surprise\n* n : neutral"},{"metadata":{},"cell_type":"markdown","source":"Load the dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the data location for SAVEE\ndir_list = os.listdir(SAVEE)\n\n#PARSE THE FILENAME TO GET TO EMOTIONS\nemotion = []\npath = []\n\nfor i in dir_list : \n    if i[-8 : -6] == \"_a\" : \n        emotion.append('male_angry')\n    elif i[-8 : -6] == \"_d\" : \n        emotion.append('male_disgust')\n    elif i[-8 : -6] == \"_h\" : \n        emotion.append('male_happy')\n    elif i[-8 : -6] == \"_f\" : \n        emotion.append('male_fear')    \n    elif i[-8 : -6] == \"sa\" : \n        emotion.append('male_sad')\n    elif i[-8 : -6] == \"su\" : \n        emotion.append('male_surprise')\n    elif i[-8 : -6] == \"_n\" : \n        emotion.append('male_neutral')\n    else:\n        emotion.append('male_error')\n    path.append(SAVEE + i)\n\n\n\n#check label distribution\nSAVEE_df = pd.DataFrame(emotion, columns = ['labels'])\nSAVEE_df['source'] = 'SAVEE'\nSAVEE_df = pd.concat([SAVEE_df, pd.DataFrame(path, columns = ['path'])],axis = 1)\nSAVEE_df.labels.value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# data exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"fname = SAVEE + 'KL_n27.wav'\ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15,3))\nlibrosa.display.waveplot(data, sr = sampling_rate)\n#play the audio\nipd.Audio(fname)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"it can be seen from waveplot speech is clear and there is no background noice or very less noise play with other sound tracks also and check background noise and speech clearity"},{"metadata":{},"cell_type":"markdown","source":"# RAVDEES DATASET"},{"metadata":{},"cell_type":"markdown","source":"\n* Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n* Vocal channel (01 = speech, 02 = song).\n* Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n* Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n* Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n* Repetition (01 = 1st repetition, 02 = 2nd repetition).\n* Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n\n03-01-01-01-01-01-01.wav MEANS 03 audio only, 01 speech, 01 neutral, 01 normal intensity, 01 statement kids, 01 1st repeat and 01  means 1st actor"},{"metadata":{"trusted":true},"cell_type":"code","source":"dir_list = os.listdir(RAV)\ndir_list.sort()\n\nemotion = []\ngender = []\npath = []\n\nfor i in dir_list: \n    fname = os.listdir(RAV + i)\n    for f in fname: \n        part = f.split(\".\")[0].split('-')\n        emotion.append(int(part[2]))\n        tmp = int(part[6])\n        if tmp %2 == 0:\n            tmp = 'female'\n        else : \n            tmp = \"male\"\n        gender.append(tmp)\n        path.append(RAV + i + '/'+ f)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RAV_df = pd.DataFrame(emotion)\nRAV_df = RAV_df.replace({1 : 'neutral',2 : 'neutral', 3 : 'happy', 4 : 'sad',5 : 'angry',6 : 'fear',7 : 'disgust',8 : 'surprise' })\nRAV_df = pd.concat([pd.DataFrame(gender), RAV_df], axis = 1)\nRAV_df.columns = ['gender','emotion']\nRAV_df['labels'] = RAV_df.gender + \"_\" + RAV_df.emotion\nRAV_df = pd.concat([RAV_df, pd.DataFrame(path, columns = ['path'])], axis = 1)\nRAV_df.drop(['gender','emotion'], axis = 1)\nRAV_df.labels.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"fname = RAV + 'Actor_04/03-01-01-01-01-02-04.wav'\ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15,3))\nlibrosa.display.waveplot(data, sr = sampling_rate)\n#play the audio\nipd.Audio(fname)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from the waveform that, it is preety clear and less of background of noise and on the beginning and the end there is no speech so it is clear. "},{"metadata":{},"cell_type":"markdown","source":"# TESS DATASET\n\n\nNow load the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"dir_list = os.listdir(TESS)\ndir_list.sort()\ndir_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = []\nemotion = []\ngender = []\n\n\nfor i in dir_list:\n    fname = os.listdir(TESS + i)\n    for f in fname: \n        part = f.split(\".\")[0].split('_')\n        tmp = part[0]\n        if tmp == 'OAF':\n            tmp = 'male'\n        else : \n            tmp = 'female'\n        gender.append(tmp)\n        emotion.append(part[2])\n        path.append(TESS + i + '/' + f)\n\nTESS_df = pd.DataFrame(emotion)\nTESS_df = pd.concat([pd.DataFrame(gender), TESS_df], axis = 1)\nTESS_df.columns = ['gender','emotion']\nTESS_df['labels'] = TESS_df.gender + \"_\" + TESS_df.emotion\nTESS_df['source'] = TESS\nTESS_df = pd.concat([TESS_df, pd.DataFrame(path,columns = ['path'])], axis = 1)\nTESS_df = TESS_df.drop(['gender', 'emotion','source'], axis = 1)\nTESS_df[\"labels\"].replace({\"male_ps\": \"male_surprise\", \"female_ps\": \"female_surprise\"}, inplace=True)\nTESS_df.labels.value_counts()\n        \n        \n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"fname = TESS + '/OAF_Sad/OAF_bar_sad.wav'\ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15,3))\nlibrosa.display.waveplot(data, sr = sampling_rate)\n#play the audio\nipd.Audio(fname)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CREMA D Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"dir_list = os.listdir(CREMA)\ndir_list.sort()\nprint(dir_list[0:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gender = []\nemotion = []\npath = []\nfemale = [1002,1003,1004,1006,1007,1008,1009,1010,1012,1013,1018,1020,1021,1024,1025,1028,1029,1030,1037,1043,1046,1047,1049,\n          1052,1053,1054,1055,1056,1058,1060,1061,1063,1072,1073,1074,1075,1076,1078,1079,1082,1084,1089,1091]\n\nfor i in dir_list: \n    part = i.split('_')\n    if int(part[0]) in female:\n        temp = 'female'\n    else:\n        temp = 'male'\n    gender.append(temp)\n    if part[2] == 'SAD' and temp == 'male':\n        emotion.append('male_sad')\n    elif part[2] == 'ANG' and temp == 'male':\n        emotion.append('male_angry')\n    elif part[2] == 'DIS' and temp == 'male':\n        emotion.append('male_disgust')\n    elif part[2] == 'FEA' and temp == 'male':\n        emotion.append('male_fear')\n    elif part[2] == 'HAP' and temp == 'male':\n        emotion.append('male_happy')\n    elif part[2] == 'NEU' and temp == 'male':\n        emotion.append('male_neutral')\n    elif part[2] == 'SAD' and temp == 'female':\n        emotion.append('female_sad')\n    elif part[2] == 'ANG' and temp == 'female':\n        emotion.append('female_angry')\n    elif part[2] == 'DIS' and temp == 'female':\n        emotion.append('female_disgust')\n    elif part[2] == 'FEA' and temp == 'female':\n        emotion.append('female_fear')\n    elif part[2] == 'HAP' and temp == 'female':\n        emotion.append('female_happy')\n    elif part[2] == 'NEU' and temp == 'female':\n        emotion.append('female_neutral')\n    else:\n        emotion.append('Unknown')\n    path.append(CREMA + i)\n    \nCREMA_df = pd.DataFrame(emotion, columns = ['labels'])\nCREMA_df['source'] = 'CREMA'\nCREMA_df = pd.concat([CREMA_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nCREMA_df.labels.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"fname = CREMA + '/1001_DFA_SAD_XX.wav'\ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15,3))\nlibrosa.display.waveplot(data, sr = sampling_rate)\n#play the audio\nipd.Audio(fname)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([SAVEE_df, RAV_df, TESS_df, CREMA_df], axis = 0)\ndf.drop(['gender','emotion'], axis = 1)\nprint(df.labels.value_counts())\ndf.to_csv(\"Data_path1.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Extraction\n\nThere are two type of features 1. Time domain and 2nd is Frequency domain feature \n\nTime domain are simple to extract and understand, like energy of signal, zero crossing rate, maximum amplitude and minimum energy. Whereas frequency domain features are obtained by converting the time based signal into frequency domain and they are harder to comprehend and it provides the extra information like pitch , rhymes, meloduy etc. "},{"metadata":{},"cell_type":"markdown","source":"# MFCC\n\nMel frequency ceptral cofficient it is a good representation of vocal tract like X ray.  In machine learning applications MFCC are treated as the image representation of sound track and it can give more information and it has an ability to draw on transfer learning. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking out MFCC \n#source : RAVDESS, Gender  : Female and Emotion : Angry\npath = \"../input/ravdess-emotional-speech-audio/Actor_08/03-01-05-01-01-01-08.wav\"\nX, sample_rate = librosa.load(path, res_type= 'kaiser_fast', duration = 2.5,sr = 22050*2, offset = 0.5)\nmfcc = librosa.feature.mfcc(y = X, sr = sample_rate, n_mfcc = 13)\n\n#audio wave graph\n\nplt.figure(figsize = (10,5))\nplt.subplot(3,1,1)\nlibrosa.display.waveplot(X, sr = sample_rate)\nplt.title('audio sampled at 44100 hrz')\n\n#MFCC\n\nplt.figure(figsize = (10,5))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis = 'time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"../input/ravdess-emotional-speech-audio/Actor_06/03-01-03-01-01-01-06.wav\"\nX, sample_rate = librosa.load(path, res_type = 'kaiser_fast', duration = 2.5, sr = 22050*2, offset = 0.5)\nmfcc = librosa.feature.mfcc(y = X, sr = sample_rate, n_mfcc = 13)\n\n#audio waveform\nplt.figure(figsize=(10,5))\nplt.subplot(3,1,1)\nlibrosa.display.waveplot(X, sr = sample_rate)\nplt.title(\"audio sampled at 44100 hz\")\n\n#mfcc waveforms\nplt.figure(figsize = (10,5))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis = 'time')\nplt.ylabel('MFCC')\n\nipd.Audio(path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# statistical features\n\nhere we are taking a mean across the each band over time and it will give us distinctive feature. if we see the MFCC plot, first band at the bottom is the most distinctive band and we are taking the shorter time window it means that, changes observed overtime does not vary greatly. So key feature is to capturing the information contained in the various bands and display it as a time series plot to illustrate the point."},{"metadata":{"trusted":true},"cell_type":"code","source":"# comparing male and female angry for the same sentence uttered. \npath = \"../input/ravdess-emotional-speech-audio/Actor_08/03-01-05-01-01-01-08.wav\"\nX, sample_rate = librosa.load(path, res_type= 'kaiser_fast', duration = 2.5, sr = 22050*2, offset = 0.5)\nfemale = librosa.feature.mfcc(y= X, sr = sample_rate, n_mfcc = 13)\nfemale = np.mean(librosa.feature.mfcc(y = X, sr = sample_rate, n_mfcc = 13), axis = 0)\nprint(len(female))\n\n# comparing male and female angry for the same sentence uttered. \npath = \"../input/ravdess-emotional-speech-audio/Actor_06/03-01-03-01-01-01-06.wav\"\nX, sample_rate = librosa.load(path, res_type= 'kaiser_fast', duration = 2.5, sr = 22050*2, offset = 0.5)\nmale = librosa.feature.mfcc(y= X, sr = sample_rate, n_mfcc = 13)\nmale = np.mean(librosa.feature.mfcc(y = X, sr = sample_rate, n_mfcc = 13), axis = 0)\nprint(len(male))\n\n#audio wave\nplt.figure(figsize = (20,15))\nplt.subplot(3,1,1)\nplt.plot(female, label = 'female')\n\nplt.plot(male, label = 'male')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"here we can see that for the same sentence spoken by male and female, female has higher pitch. "},{"metadata":{},"cell_type":"markdown","source":"## Model Creation "},{"metadata":{},"cell_type":"markdown","source":"Importing required libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras import regularizers\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\nfrom keras.layers import Conv1D, MaxPool1D, AveragePooling1D\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import ModelCheckpoint\n\n#sklearn\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ref = pd.read_csv('./Data_path1.csv')\nref.drop(['gender','emotion'], axis = 1, inplace = True)\nref.head()\nprint(ref.labels.value_counts())\nref.to_csv(\"Data_path2.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now we will read all the audio files , extract its mean across all MFCC bands by time and just keep extracting the features, dropping entire audio file data"},{"metadata":{"trusted":true},"cell_type":"code","source":"ref1 = pd.read_csv('./Data_path2.csv')\nref1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(columns = ['features'])\n# looping feature extraction over entire dataset\n\ncounter = 0\nfor index,path in enumerate(ref1.path):\n    X, sample_rate = librosa.load(path, res_type='kaiser_fast', duration = 2.5, sr = 22050*2, offset = 0.5)\n    sample_rate = np.array(sample_rate)\n    \n    #taking mean of MFCC \n    mfccs = np.mean(librosa.feature.mfcc(y = X, sr = sampling_rate, n_mfcc = 13), axis = 0)\n    df.loc[counter] = [mfccs]\n    counter = counter+1 \nprint(len(df))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([ref1, pd.DataFrame(df['features']. values.tolist())], axis = 1)\ndf[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"replace na with 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.fillna(0)\nprint(df.shape)\ndf[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting the data in 2 parts: training and validation and measuring models performance and accuracy "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df.drop(['path','labels','source'], axis = 1), df.labels, test_size = 0.25, shuffle = True, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[150 : 160]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will convert the data format to numpy array and implement a 1D CNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = np.array(X_train)\ny_train = np.array(y_train)\nX_test = np.array(X_test)\ny_test = np.array(y_test)\n\n#one hot encode target \nlb = LabelEncoder()\ny_train = np_utils.to_categorical(lb.fit_transform(y_train))\ny_test = np_utils.to_categorical(lb.fit_transform(y_test))\n\nprint(X_train.shape)\nprint(lb.classes_)\n\n#pickle lb object for future use\n\nfilename = 'labels'\noutfile = open(filename, 'wb')\npickle.dump(lb,outfile)\noutfile.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we are using 1D CNN, so we need to expand our dimention to 3rd dimention"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = np.expand_dims(X_train, axis = 2)\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = np.expand_dims(X_test, axis = 2)\nX_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#New Model\n\nmodel = Sequential()\nmodel.add(Conv1D(256, 8, padding = 'same', input_shape = (X_train.shape[1],1))) \nmodel.add(Activation('relu'))\nmodel.add(Conv1D(256,8,padding = 'same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPool1D(pool_size = (8)))\nmodel.add(Conv1D(128, 8 , padding = 'same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8 , padding = 'same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8 , padding = 'same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8 , padding = 'same'))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPool1D(pool_size = (8)))\nmodel.add(Conv1D(64, 8 , padding = 'same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(64, 8 , padding = 'same'))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(14)) # no of target class\nmodel.add(Activation('softmax'))\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"opt = keras.optimizers.RMSprop(learning_rate = 0.00001, decay = 1e-6)\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\nhistory = model.fit(X_train,y_train, batch_size = 16, epochs = 100, validation_data = (X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title(\"model loss\")\nplt.ylabel(\"loss\")\nplt.xlabel('epochs')\nplt.legend(['train','test'],loc = 'upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# save the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name = 'Emotion_recognition.h5'\nsave_dir = os.path.join(os.getcwd(),'saved_models')\nif not os.path.isdir(save_dir):\n    os.mkdir(save_dir)\nmodel_path = os.path.join(save_dir,model_name)\nmodel.save(model_path)\nprint(\"save model and weight at %s\" %model_path)\n#save model to disk\nmodel_json = model.to_json()\nwith open(\"model_json.json\", \"w\") as file:\n    file.write(model_json)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}